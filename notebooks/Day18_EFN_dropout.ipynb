{"cells":[{"cell_type":"code","execution_count":1,"source":["from tensorflow.keras.callbacks import LearningRateScheduler\r\n","from tensorflow.keras.preprocessing.image import random_rotation, random_shear, random_zoom\r\n","from sklearn.metrics import confusion_matrix, classification_report\r\n","from tensorflow.keras.applications.efficientnet import preprocess_input\r\n","from tensorflow.keras.applications import EfficientNetB0\r\n","from tensorflow.keras.utils import to_categorical\r\n","from tensorflow.keras.layers import Dense, Dropout, Input\r\n","import matplotlib.pyplot as plt\r\n","import tensorflow as tf\r\n","import pandas as pd\r\n","import numpy as np\r\n","import os\r\n","import itertools\r\n","import mlflow.tensorflow\r\n","import mlflow\r\n","import cv2"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["def prepare_data(data, to_3_channels=True, to_clahe=False):\r\n","    \"\"\" Prepare data for modeling\r\n","        input: data frame with labels and pixel data\r\n","        output: image and label array in shape(48,48,3) and pixel range(0,256) \"\"\"\r\n","    clahe = cv2.createCLAHE(clipLimit=2)\r\n","    channels = 3 if to_3_channels == True else 1\r\n","\r\n","    image_array = np.zeros(shape=(len(data), 48, 48, channels))\r\n","    image_label = np.array(list(map(int, data['emotion'])))\r\n","\r\n","    for i, row in enumerate(data.index):\r\n","        image = np.fromstring(data.loc[row, 'pixels'], dtype=int, sep=' ')\r\n","        image = np.reshape(image, (48, 48, 1))  # 灰階圖的channel數為1\r\n","\r\n","        #  CLAHE (Contrast Limited Adaptive Histogram Equalization)\r\n","        if to_clahe == True:\r\n","            image = image[:, :, 0].astype(\"uint8\")\r\n","            image = clahe.apply(image)\r\n","            image = np.reshape(image, (48, 48, 1))\r\n","\r\n","        # Convert to 3 channels\r\n","        if to_3_channels == True:\r\n","            image = np.stack(\r\n","                [image[:, :, 0], image[:, :, 0], image[:, :, 0]], axis=-1)\r\n","        image_processed = preprocess_input(image)\r\n","        image_array[i] = image_processed\r\n","\r\n","    return image_array, image_label\r\n","\r\n","\r\n","def build_model(preModel=EfficientNetB0, num_classes=7):\r\n","\r\n","    pre_model = preModel(include_top=False, weights='imagenet',\r\n","                         input_shape=(48, 48, 3),\r\n","                         pooling='max', classifier_activation='softmax')\r\n","\r\n","    x = Dropout(0.2)(pre_model.output)\r\n","\r\n","    output = Dense(\r\n","        num_classes, activation=\"softmax\", name=\"main_output\")(x)\r\n","\r\n","    model = tf.keras.Model(\r\n","        pre_model.input, output)\r\n","\r\n","    model.compile(optimizer=tf.keras.optimizers.Adam(),\r\n","                  loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\r\n","\r\n","    return model\r\n","\r\n","\r\n","def resize_image(img_array, output_shape=(224, 224)):\r\n","    output_img = cv2.resize(img_array, output_shape)\r\n","    return output_img\r\n","\r\n","\r\n","def augmentation_image(img_array):\r\n","    tf.random.set_seed(19960220)\r\n","    img_array = random_rotation(img_array, rg=30, channel_axis=2)  # 旋轉\r\n","    img_array = random_shear(img_array, intensity=20, channel_axis=2)  # 剪裁\r\n","    img_array = random_zoom(img_array, zoom_range=(\r\n","        0.8, 0.8), channel_axis=2)  # 縮放\r\n","    return img_array\r\n","\r\n","\r\n","def auto_augmentation(X_train, y_train, class_sample_size, ratio=1):\r\n","    max_class_size = np.max(class_sample_size)\r\n","    fill_class_sample_size = [int(ratio*max_class_size - size)\r\n","                              for size in class_sample_size]\r\n","    X_train_aug_array = []\r\n","    y_train_aug_array = []\r\n","    for i, fill_size in enumerate(fill_class_sample_size):\r\n","        samples = np.random.choice(list(np.where(y_train == i)[0]), fill_size)\r\n","        for image in X_train[samples]:\r\n","            image_aug = augmentation_image(image)\r\n","            X_train_aug_array.append(image_aug)\r\n","            y_train_aug_array.append(i)\r\n","    X_train_aug_array = np.array(X_train_aug_array)\r\n","    y_train_aug_array = np.array(y_train_aug_array)\r\n","    return X_train_aug_array, y_train_aug_array\r\n","\r\n","\r\n","def plot_one_emotion(data, img_arrays, img_labels, label=0):\r\n","    fig, axs = plt.subplots(1, 5, figsize=(25, 12))\r\n","    fig.subplots_adjust(hspace=.2, wspace=.2)\r\n","    axs = axs.ravel()\r\n","    for i in range(5):\r\n","        idx = data[data['emotion'] == label].index[i]\r\n","        axs[i].imshow(img_arrays[idx][:, :, 0], cmap='gray')\r\n","        axs[i].set_title(emotions[img_labels[idx]])\r\n","        axs[i].set_xticklabels([])\r\n","        axs[i].set_yticklabels([])\r\n","\r\n","\r\n","def step_decay(epoch):\r\n","    \"\"\"\r\n","    Warm-up applying high learning rate at first few epochs.\r\n","    Step decay schedule drops the learning rate by a factor every few epochs.\r\n","    \"\"\"\r\n","    lr_init = 0.001\r\n","    drop = 0.5\r\n","    epochs_drop = 5\r\n","    warm_up_epoch = 0\r\n","    if epoch+1 < warm_up_epoch:  # warm_up_epoch之前採用warmup\r\n","        lr = drop * ((epoch+1) / warm_up_epoch)\r\n","    else:  # 每epochs_drop個epoch，lr乘以drop倍。\r\n","        lr = lr_init * (drop**(int(((1+epoch)/epochs_drop))))\r\n","    return lr\r\n","\r\n","\r\n","def exp_decay(epoch):\r\n","    \"\"\"\r\n","    Warm-up applying high learning rate at first few epochs.\r\n","    \"\"\"\r\n","    lr_init = 0.001\r\n","    lr = lr_init * tf.math.exp(-0.1 * (epoch+1))\r\n","    return lr\r\n","\r\n","\r\n","def poly_decay(epoch):\r\n","    \"\"\"\r\n","    Warm-up applying high learning rate at first few epochs.\r\n","    \"\"\"\r\n","    lr_init = 0.001\r\n","    lr_end = 0.0001\r\n","    decay_steps = 10\r\n","    global_step = epoch\r\n","    power = 0.5\r\n","    if global_step > decay_steps:\r\n","        global_step = global_step % decay_steps\r\n","\r\n","    lr = (lr_init-lr_end)*((1-(global_step/decay_steps))**power) + lr_end\r\n","    return float(lr)\r\n","\r\n","\r\n","emotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear',\r\n","            3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["df_raw = pd.read_csv(\"D:/mycodes/AIFER/data/FER2013/fer2013.csv\")\r\n","#  資料前處理(CLAHE)\r\n","X_train, y_train = prepare_data(df_raw[df_raw['Usage'] == 'Training'])\r\n","X_val, y_val = prepare_data(df_raw[df_raw['Usage'] == 'PublicTest'])"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["model = build_model()\r\n","prob_res = model(X_train[:1]).numpy()\r\n","print(f\"EFN build successfully!\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["EFN build successfully!\n"]}],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["epochs = 30\r\n","batch_size = 32\r\n","model = build_model()\r\n","y_train_oh, y_val_oh = to_categorical(y_train), to_categorical(y_val)\r\n","with mlflow.start_run(experiment_id=1, run_name=\"dropout\"):\r\n","    mlflow.tensorflow.autolog()\r\n","    hist1 = model.fit(X_train, y_train_oh, validation_data=(X_val, y_val_oh),\r\n","                      epochs=epochs, batch_size=batch_size)\r\n","mlflow.end_run()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","  1/898 [..............................] - ETA: 0s - loss: 2.8194 - accuracy: 0.2812WARNING:tensorflow:From C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n","Instructions for updating:\n","use `tf.profiler.experimental.stop` instead.\n","  2/898 [..............................] - ETA: 2:30 - loss: 2.7406 - accuracy: 0.2812WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0539s vs `on_train_batch_end` time: 0.2631s). Check your callbacks.\n","898/898 [==============================] - 69s 77ms/step - loss: 1.5058 - accuracy: 0.4492 - val_loss: 1.2150 - val_accuracy: 0.5364\n","Epoch 2/30\n","898/898 [==============================] - 71s 79ms/step - loss: 1.1590 - accuracy: 0.5645 - val_loss: 1.1253 - val_accuracy: 0.5854\n","Epoch 3/30\n","898/898 [==============================] - 80s 89ms/step - loss: 1.0454 - accuracy: 0.6116 - val_loss: 1.0930 - val_accuracy: 0.6077\n","Epoch 4/30\n","898/898 [==============================] - 70s 78ms/step - loss: 0.9599 - accuracy: 0.6472 - val_loss: 1.0887 - val_accuracy: 0.5971\n","Epoch 5/30\n","898/898 [==============================] - 81s 90ms/step - loss: 0.8848 - accuracy: 0.6734 - val_loss: 1.0663 - val_accuracy: 0.6172\n","Epoch 6/30\n","898/898 [==============================] - 97s 108ms/step - loss: 0.8047 - accuracy: 0.7068 - val_loss: 1.0932 - val_accuracy: 0.6138\n","Epoch 7/30\n","898/898 [==============================] - 88s 98ms/step - loss: 0.7201 - accuracy: 0.7385 - val_loss: 1.1310 - val_accuracy: 0.6082\n","Epoch 8/30\n","898/898 [==============================] - 90s 100ms/step - loss: 0.6331 - accuracy: 0.7689 - val_loss: 1.1913 - val_accuracy: 0.6202\n","Epoch 9/30\n","898/898 [==============================] - 99s 110ms/step - loss: 0.5564 - accuracy: 0.7977 - val_loss: 1.2248 - val_accuracy: 0.6155\n","Epoch 10/30\n","898/898 [==============================] - 82s 92ms/step - loss: 0.4888 - accuracy: 0.8242 - val_loss: 1.2809 - val_accuracy: 0.6230\n","Epoch 11/30\n","898/898 [==============================] - 99s 110ms/step - loss: 0.4291 - accuracy: 0.8482 - val_loss: 1.3196 - val_accuracy: 0.6119\n","Epoch 12/30\n","898/898 [==============================] - 98s 110ms/step - loss: 0.3768 - accuracy: 0.8661 - val_loss: 1.5309 - val_accuracy: 0.6250\n","Epoch 13/30\n","898/898 [==============================] - 91s 101ms/step - loss: 0.3432 - accuracy: 0.8793 - val_loss: 1.4541 - val_accuracy: 0.6233\n","Epoch 14/30\n","898/898 [==============================] - 78s 87ms/step - loss: 0.3250 - accuracy: 0.8853 - val_loss: 1.4751 - val_accuracy: 0.6239\n","Epoch 15/30\n","898/898 [==============================] - 78s 87ms/step - loss: 0.2654 - accuracy: 0.9073 - val_loss: 1.5451 - val_accuracy: 0.6038\n","Epoch 16/30\n","898/898 [==============================] - 74s 83ms/step - loss: 0.3030 - accuracy: 0.8950 - val_loss: 1.5303 - val_accuracy: 0.6350\n","Epoch 17/30\n","898/898 [==============================] - 89s 99ms/step - loss: 0.2431 - accuracy: 0.9160 - val_loss: 1.6933 - val_accuracy: 0.6141\n","Epoch 18/30\n","898/898 [==============================] - 86s 95ms/step - loss: 0.2180 - accuracy: 0.9247 - val_loss: 1.7080 - val_accuracy: 0.6144\n","Epoch 19/30\n","898/898 [==============================] - 84s 94ms/step - loss: 0.2466 - accuracy: 0.9162 - val_loss: 2.1880 - val_accuracy: 0.5756\n","Epoch 20/30\n","898/898 [==============================] - 80s 89ms/step - loss: 0.2519 - accuracy: 0.9124 - val_loss: 1.6942 - val_accuracy: 0.6030\n","Epoch 21/30\n","898/898 [==============================] - 71s 79ms/step - loss: 0.1771 - accuracy: 0.9383 - val_loss: 1.7963 - val_accuracy: 0.6236\n","Epoch 22/30\n","898/898 [==============================] - 78s 87ms/step - loss: 0.3095 - accuracy: 0.8941 - val_loss: 1.5983 - val_accuracy: 0.6152\n","Epoch 23/30\n","898/898 [==============================] - 85s 95ms/step - loss: 0.1883 - accuracy: 0.9362 - val_loss: 1.7079 - val_accuracy: 0.6300\n","Epoch 24/30\n","898/898 [==============================] - 100s 111ms/step - loss: 0.1523 - accuracy: 0.9466 - val_loss: 1.7726 - val_accuracy: 0.6094\n","Epoch 25/30\n","898/898 [==============================] - 116s 129ms/step - loss: 0.2251 - accuracy: 0.9232 - val_loss: 1.9129 - val_accuracy: 0.6041\n","Epoch 26/30\n","898/898 [==============================] - 85s 94ms/step - loss: 0.1519 - accuracy: 0.9476 - val_loss: 1.7670 - val_accuracy: 0.6188\n","Epoch 27/30\n","898/898 [==============================] - 93s 103ms/step - loss: 0.1484 - accuracy: 0.9480 - val_loss: 1.8287 - val_accuracy: 0.6155\n","Epoch 28/30\n","898/898 [==============================] - 75s 84ms/step - loss: 0.1538 - accuracy: 0.9482 - val_loss: 1.8187 - val_accuracy: 0.6177\n","Epoch 29/30\n","898/898 [==============================] - 75s 84ms/step - loss: 0.1617 - accuracy: 0.9446 - val_loss: 1.8409 - val_accuracy: 0.6325\n","Epoch 30/30\n","898/898 [==============================] - 77s 86ms/step - loss: 0.1358 - accuracy: 0.9534 - val_loss: 1.8655 - val_accuracy: 0.6347\n","WARNING:tensorflow:From C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","WARNING:tensorflow:From C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","INFO:tensorflow:Assets written to: C:\\Users\\USER\\AppData\\Local\\Temp\\tmpv7eo0z5f\\model\\data\\model\\assets\n"]}],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["1"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":6}],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}