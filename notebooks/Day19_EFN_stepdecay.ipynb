{"cells":[{"cell_type":"code","execution_count":1,"source":["from tensorflow.keras.callbacks import LearningRateScheduler\n","from tensorflow.keras.preprocessing.image import random_rotation, random_shear, random_zoom\n","from sklearn.metrics import confusion_matrix, classification_report\n","from tensorflow.keras.applications.efficientnet import preprocess_input\n","from tensorflow.keras.applications import EfficientNetB0\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Dense, Dropout, Input\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import os\n","import itertools\n","import mlflow.tensorflow\n","import mlflow\n","import cv2"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["def prepare_data(data, to_3_channels=True, to_clahe=False):\n","    \"\"\" Prepare data for modeling\n","        input: data frame with labels and pixel data\n","        output: image and label array in shape(48,48,3) and pixel range(0,256) \"\"\"\n","    clahe = cv2.createCLAHE(clipLimit=2)\n","    channels = 3 if to_3_channels == True else 1\n","\n","    image_array = np.zeros(shape=(len(data), 48, 48, channels))\n","    image_label = np.array(list(map(int, data['emotion'])))\n","\n","    for i, row in enumerate(data.index):\n","        image = np.fromstring(data.loc[row, 'pixels'], dtype=int, sep=' ')\n","        image = np.reshape(image, (48, 48, 1))  # 灰階圖的channel數為1\n","\n","        #  CLAHE (Contrast Limited Adaptive Histogram Equalization)\n","        if to_clahe == True:\n","            image = image[:, :, 0].astype(\"uint8\")\n","            image = clahe.apply(image)\n","            image = np.reshape(image, (48, 48, 1))\n","\n","        # Convert to 3 channels\n","        if to_3_channels == True:\n","            image = np.stack(\n","                [image[:, :, 0], image[:, :, 0], image[:, :, 0]], axis=-1)\n","        image_processed = preprocess_input(image)\n","        image_array[i] = image_processed\n","\n","    return image_array, image_label\n","\n","\n","def build_model(preModel=EfficientNetB0, num_classes=7):\n","\n","    pre_model = preModel(include_top=False, weights='imagenet',\n","                         input_shape=(48, 48, 3),\n","                         pooling='max', classifier_activation='softmax')\n","\n","    output_layer = Dense(\n","        num_classes, activation=\"softmax\", name=\"main_output\")\n","\n","    model = tf.keras.Model(\n","        pre_model.input, output_layer(pre_model.output))\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(),\n","                  loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n","\n","    return model\n","\n","\n","def resize_image(img_array, output_shape=(224, 224)):\n","    output_img = cv2.resize(img_array, output_shape)\n","    return output_img\n","\n","\n","def augmentation_image(img_array):\n","    tf.random.set_seed(19960220)\n","    img_array = random_rotation(img_array, rg=30, channel_axis=2)  # 旋轉\n","    img_array = random_shear(img_array, intensity=20, channel_axis=2)  # 剪裁\n","    img_array = random_zoom(img_array, zoom_range=(\n","        0.8, 0.8), channel_axis=2)  # 縮放\n","    return img_array\n","\n","\n","def auto_augmentation(X_train, y_train, class_sample_size, ratio=1):\n","    max_class_size = np.max(class_sample_size)\n","    fill_class_sample_size = [int(ratio*max_class_size - size)\n","                              for size in class_sample_size]\n","    X_train_aug_array = []\n","    y_train_aug_array = []\n","    for i, fill_size in enumerate(fill_class_sample_size):\n","        samples = np.random.choice(list(np.where(y_train == i)[0]), fill_size)\n","        for image in X_train[samples]:\n","            image_aug = augmentation_image(image)\n","            X_train_aug_array.append(image_aug)\n","            y_train_aug_array.append(i)\n","    X_train_aug_array = np.array(X_train_aug_array)\n","    y_train_aug_array = np.array(y_train_aug_array)\n","    return X_train_aug_array, y_train_aug_array\n","\n","\n","def plot_one_emotion(data, img_arrays, img_labels, label=0):\n","    fig, axs = plt.subplots(1, 5, figsize=(25, 12))\n","    fig.subplots_adjust(hspace=.2, wspace=.2)\n","    axs = axs.ravel()\n","    for i in range(5):\n","        idx = data[data['emotion'] == label].index[i]\n","        axs[i].imshow(img_arrays[idx][:, :, 0], cmap='gray')\n","        axs[i].set_title(emotions[img_labels[idx]])\n","        axs[i].set_xticklabels([])\n","        axs[i].set_yticklabels([])\n","\n","\n","def step_decay(epoch):\n","    \"\"\"\n","    Warm-up applying high learning rate at first few epochs.\n","    Step decay schedule drops the learning rate by a factor every few epochs.\n","    \"\"\"\n","    lr_init = 0.001\n","    drop = 0.5\n","    epochs_drop = 5\n","    warm_up_epoch = 0\n","    if epoch+1 < warm_up_epoch:  # warm_up_epoch之前採用warmup\n","        lr = drop * ((epoch+1) / warm_up_epoch)\n","    else:  # 每epochs_drop個epoch，lr乘以drop倍。\n","        lr = lr_init * (drop**(int(((1+epoch)/epochs_drop))))\n","    return lr\n","\n","\n","def exponential_decay(epoch):\n","    \"\"\"\n","    Warm-up applying high learning rate at first few epochs.\n","    Step decay schedule drops the learning rate by a factor every few epochs.\n","    \"\"\"\n","    lr_init = 0.001\n","    lr = lr_init * tf.math.exp(-0.1 * (epoch+1))\n","    return lr\n","\n","\n","def polynomial_decay(epoch):\n","    \"\"\"\n","    Warm-up applying high learning rate at first few epochs.\n","    Step decay schedule drops the learning rate by a factor every few epochs.\n","    \"\"\"\n","    lr_init = 0.001\n","    lr_end = 0.0001\n","    global_step = epoch+1\n","    decay_steps = 10\n","    lr = (lr_init-lr_end)*((1-(global_step/decay_steps))**2) + lr_end\n","    return lr\n","\n","\n","emotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear',\n","            3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["def prepare_data(data, to_3_channels=True, to_clahe=False):\n","    \"\"\" Prepare data for modeling\n","        input: data frame with labels and pixel data\n","        output: image and label array in shape(48,48,3) and pixel range(0,256) \"\"\"\n","    clahe = cv2.createCLAHE(clipLimit=2)\n","    channels = 3 if to_3_channels == True else 1\n","\n","    image_array = np.zeros(shape=(len(data), 48, 48, channels))\n","    image_label = np.array(list(map(int, data['emotion'])))\n","\n","    for i, row in enumerate(data.index):\n","        image = np.fromstring(data.loc[row, 'pixels'], dtype=int, sep=' ')\n","        image = np.reshape(image, (48, 48, 1))  # 灰階圖的channel數為1\n","\n","        #  CLAHE (Contrast Limited Adaptive Histogram Equalization)\n","        if to_clahe == True:\n","            image = image[:, :, 0].astype(\"uint8\")\n","            image = clahe.apply(image)\n","            image = np.reshape(image, (48, 48, 1))\n","\n","        # Convert to 3 channels\n","        if to_3_channels == True:\n","            image = np.stack(\n","                [image[:, :, 0], image[:, :, 0], image[:, :, 0]], axis=-1)\n","        image_processed = preprocess_input(image)\n","        image_array[i] = image_processed\n","\n","    return image_array, image_label\n","\n","\n","def build_model(preModel=EfficientNetB0, num_classes=7):\n","\n","    pre_model = preModel(include_top=False, weights='imagenet',\n","                         input_shape=(48, 48, 3),\n","                         pooling='max', classifier_activation='softmax')\n","\n","    output_layer = Dense(\n","        num_classes, activation=\"softmax\", name=\"main_output\")\n","\n","    model = tf.keras.Model(\n","        pre_model.input, output_layer(pre_model.output))\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(),\n","                  loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n","\n","    return model\n","\n","\n","def resize_image(img_array, output_shape=(224, 224)):\n","    output_img = cv2.resize(img_array, output_shape)\n","    return output_img\n","\n","\n","def augmentation_image(img_array):\n","    tf.random.set_seed(19960220)\n","    img_array = random_rotation(img_array, rg=30, channel_axis=2)  # 旋轉\n","    img_array = random_shear(img_array, intensity=20, channel_axis=2)  # 剪裁\n","    img_array = random_zoom(img_array, zoom_range=(\n","        0.8, 0.8), channel_axis=2)  # 縮放\n","    return img_array\n","\n","\n","def auto_augmentation(X_train, y_train, class_sample_size, ratio=1):\n","    max_class_size = np.max(class_sample_size)\n","    fill_class_sample_size = [int(ratio*max_class_size - size)\n","                              for size in class_sample_size]\n","    X_train_aug_array = []\n","    y_train_aug_array = []\n","    for i, fill_size in enumerate(fill_class_sample_size):\n","        samples = np.random.choice(list(np.where(y_train == i)[0]), fill_size)\n","        for image in X_train[samples]:\n","            image_aug = augmentation_image(image)\n","            X_train_aug_array.append(image_aug)\n","            y_train_aug_array.append(i)\n","    X_train_aug_array = np.array(X_train_aug_array)\n","    y_train_aug_array = np.array(y_train_aug_array)\n","    return X_train_aug_array, y_train_aug_array\n","\n","\n","def plot_one_emotion(data, img_arrays, img_labels, label=0):\n","    fig, axs = plt.subplots(1, 5, figsize=(25, 12))\n","    fig.subplots_adjust(hspace=.2, wspace=.2)\n","    axs = axs.ravel()\n","    for i in range(5):\n","        idx = data[data['emotion'] == label].index[i]\n","        axs[i].imshow(img_arrays[idx][:, :, 0], cmap='gray')\n","        axs[i].set_title(emotions[img_labels[idx]])\n","        axs[i].set_xticklabels([])\n","        axs[i].set_yticklabels([])\n","\n","\n","def step_decay(epoch):\n","    \"\"\"\n","    Warm-up applying high learning rate at first few epochs.\n","    Step decay schedule drops the learning rate by a factor every few epochs.\n","    \"\"\"\n","    lr_init = 0.001\n","    drop = 0.5\n","    epochs_drop = 5\n","    warm_up_epoch = 0\n","    if epoch+1 < warm_up_epoch:  # warm_up_epoch之前採用warmup\n","        lr = drop * ((epoch+1) / warm_up_epoch)\n","    else:  # 每epochs_drop個epoch，lr乘以drop倍。\n","        lr = lr_init * (drop**(int(((1+epoch)/epochs_drop))))\n","    return lr\n","\n","\n","def exp_decay(epoch):\n","    \"\"\"\n","    Warm-up applying high learning rate at first few epochs.\n","    Step decay schedule drops the learning rate by a factor every few epochs.\n","    \"\"\"\n","    lr_init = 0.001\n","    lr = lr_init * tf.math.exp(-0.1 * (epoch+1))\n","    return lr\n","\n","\n","def poly_decay(epoch):\n","    \"\"\"\n","    Warm-up applying high learning rate at first few epochs.\n","    Step decay schedule drops the learning rate by a factor every few epochs.\n","    \"\"\"\n","    lr_init = 0.001\n","    lr_end = 0.0001\n","    global_step = epoch+1\n","    decay_steps = 10\n","    lr = (lr_init-lr_end)*((1-(global_step/decay_steps))**2) + lr_end\n","    return lr\n","\n","\n","emotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear',\n","            3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["df_raw = pd.read_csv(\"D:/mycodes/AIFER/data/FER2013/fer2013.csv\")\n","#  資料前處理(CLAHE)\n","X_train, y_train = prepare_data(df_raw[df_raw['Usage'] == 'Training'])\n","X_val, y_val = prepare_data(df_raw[df_raw['Usage'] == 'PublicTest'])"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["df_raw = pd.read_csv(\"D:/mycodes/AIFER/data/FER2013/fer2013.csv\")\n","#  資料前處理(CLAHE)\n","X_train, y_train = prepare_data(df_raw[df_raw['Usage'] == 'Training'])\n","X_val, y_val = prepare_data(df_raw[df_raw['Usage'] == 'PublicTest'])"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["model = build_model()\n","prob_res = model(X_train[:1]).numpy()\n","print(f\"EFN build successfully!\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["EFN build successfully!\n"]}],"metadata":{}},{"cell_type":"code","execution_count":7,"source":["zip([\"step_decay\",\"exp_decay\",\"poly_decay\"],[step_decay,exp_decay,poly_decay])"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["<zip at 0x2257e4f4c88>"]},"metadata":{},"execution_count":7}],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["for i,j in zip([\"step_decay\",\"exp_decay\",\"poly_decay\"],[step_decay,exp_decay,poly_decay]):\n","    print(i,j)"],"outputs":[{"output_type":"stream","name":"stdout","text":["step_decay <function step_decay at 0x000002254BDF2168>\n","exp_decay <function exp_decay at 0x000002254BDF2288>\n","poly_decay <function poly_decay at 0x000002254BDF23A8>\n"]}],"metadata":{}},{"cell_type":"code","execution_count":9,"source":["epochs = 30\n","batch_size = 32\n","\n","for lr_name, lr_fn in zip([\"step_decay\",\"exp_decay\",\"poly_decay\"],[step_decay,exp_decay,poly_decay]):\n","    lrate = LearningRateScheduler(lr_fn)\n","    callbacks_list = [lrate]\n","    model = build_model()\n","    y_train_oh, y_val_oh = to_categorical(y_train), to_categorical(y_val)\n","    with mlflow.start_run(experiment_id=1, run_name=lr_name):\n","        mlflow.tensorflow.autolog()\n","        hist1 = model.fit(X_train, y_train_oh, validation_data=(X_val, y_val_oh),\n","                        epochs=epochs, batch_size=batch_size, callbacks=callbacks_list)\n","    mlflow.end_run()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","  1/898 [..............................] - ETA: 0s - loss: 2.6923 - accuracy: 0.0938WARNING:tensorflow:From C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n","Instructions for updating:\n","use `tf.profiler.experimental.stop` instead.\n","  2/898 [..............................] - ETA: 2:58 - loss: 2.7717 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0868s vs `on_train_batch_end` time: 0.3060s). Check your callbacks.\n","898/898 [==============================] - 78s 87ms/step - loss: 1.4766 - accuracy: 0.4521 - val_loss: 1.2289 - val_accuracy: 0.5366\n","Epoch 2/30\n","898/898 [==============================] - 68s 76ms/step - loss: 1.1437 - accuracy: 0.5711 - val_loss: 1.1050 - val_accuracy: 0.5790\n","Epoch 3/30\n","898/898 [==============================] - 66s 74ms/step - loss: 1.0294 - accuracy: 0.6189 - val_loss: 1.2344 - val_accuracy: 0.5620\n","Epoch 4/30\n","898/898 [==============================] - 66s 74ms/step - loss: 0.9360 - accuracy: 0.6528 - val_loss: 1.1204 - val_accuracy: 0.5988\n","Epoch 5/30\n","898/898 [==============================] - 66s 74ms/step - loss: 0.7236 - accuracy: 0.7335 - val_loss: 1.0584 - val_accuracy: 0.6266\n","Epoch 6/30\n","898/898 [==============================] - 67s 74ms/step - loss: 0.6049 - accuracy: 0.7791 - val_loss: 1.1365 - val_accuracy: 0.6186\n","Epoch 7/30\n","898/898 [==============================] - 65s 73ms/step - loss: 0.5027 - accuracy: 0.8167 - val_loss: 1.2492 - val_accuracy: 0.6172\n","Epoch 8/30\n","898/898 [==============================] - 66s 74ms/step - loss: 0.4107 - accuracy: 0.8533 - val_loss: 1.3140 - val_accuracy: 0.6191\n","Epoch 9/30\n","898/898 [==============================] - 66s 74ms/step - loss: 0.3371 - accuracy: 0.8776 - val_loss: 1.4561 - val_accuracy: 0.6255\n","Epoch 10/30\n","898/898 [==============================] - 66s 74ms/step - loss: 0.2054 - accuracy: 0.9272 - val_loss: 1.5894 - val_accuracy: 0.6381\n","Epoch 11/30\n","898/898 [==============================] - 67s 74ms/step - loss: 0.1491 - accuracy: 0.9480 - val_loss: 1.8024 - val_accuracy: 0.6434\n","Epoch 12/30\n","898/898 [==============================] - 67s 74ms/step - loss: 0.1296 - accuracy: 0.9560 - val_loss: 1.9218 - val_accuracy: 0.6383\n","Epoch 13/30\n","898/898 [==============================] - 67s 74ms/step - loss: 0.1198 - accuracy: 0.9603 - val_loss: 1.8561 - val_accuracy: 0.6319\n","Epoch 14/30\n","898/898 [==============================] - 67s 74ms/step - loss: 0.1085 - accuracy: 0.9618 - val_loss: 1.9949 - val_accuracy: 0.6308\n","Epoch 15/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.0782 - accuracy: 0.9738 - val_loss: 2.0507 - val_accuracy: 0.6358\n","Epoch 16/30\n","898/898 [==============================] - 67s 75ms/step - loss: 0.0689 - accuracy: 0.9763 - val_loss: 2.0339 - val_accuracy: 0.6364\n","Epoch 17/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.0578 - accuracy: 0.9801 - val_loss: 2.1366 - val_accuracy: 0.6314\n","Epoch 18/30\n","898/898 [==============================] - 69s 77ms/step - loss: 0.0576 - accuracy: 0.9810 - val_loss: 2.1669 - val_accuracy: 0.6325\n","Epoch 19/30\n","898/898 [==============================] - 67s 74ms/step - loss: 0.0544 - accuracy: 0.9813 - val_loss: 2.1431 - val_accuracy: 0.6383\n","Epoch 20/30\n","898/898 [==============================] - 68s 75ms/step - loss: 0.0425 - accuracy: 0.9859 - val_loss: 2.2291 - val_accuracy: 0.6395\n","Epoch 21/30\n","898/898 [==============================] - 67s 74ms/step - loss: 0.0413 - accuracy: 0.9872 - val_loss: 2.2578 - val_accuracy: 0.6319\n","Epoch 22/30\n","898/898 [==============================] - 70s 78ms/step - loss: 0.0386 - accuracy: 0.9865 - val_loss: 2.2297 - val_accuracy: 0.6322\n","Epoch 23/30\n","898/898 [==============================] - 67s 74ms/step - loss: 0.0381 - accuracy: 0.9877 - val_loss: 2.2813 - val_accuracy: 0.6375\n","Epoch 24/30\n","898/898 [==============================] - 70s 78ms/step - loss: 0.0354 - accuracy: 0.9878 - val_loss: 2.3513 - val_accuracy: 0.6397\n","Epoch 25/30\n","898/898 [==============================] - 69s 77ms/step - loss: 0.0309 - accuracy: 0.9894 - val_loss: 2.3411 - val_accuracy: 0.6403\n","Epoch 26/30\n","898/898 [==============================] - 70s 78ms/step - loss: 0.0274 - accuracy: 0.9900 - val_loss: 2.3889 - val_accuracy: 0.6375\n","Epoch 27/30\n","898/898 [==============================] - 69s 76ms/step - loss: 0.0288 - accuracy: 0.9893 - val_loss: 2.4062 - val_accuracy: 0.6361\n","Epoch 28/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.0282 - accuracy: 0.9902 - val_loss: 2.4264 - val_accuracy: 0.6375\n","Epoch 29/30\n","898/898 [==============================] - 70s 78ms/step - loss: 0.0308 - accuracy: 0.9898 - val_loss: 2.3645 - val_accuracy: 0.6439\n","Epoch 30/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.0256 - accuracy: 0.9913 - val_loss: 2.3831 - val_accuracy: 0.6381\n","WARNING:tensorflow:From C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","WARNING:tensorflow:From C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","INFO:tensorflow:Assets written to: C:\\Users\\USER\\AppData\\Local\\Temp\\tmpozim3ztw\\model\\data\\model\\assets\n","Epoch 1/30\n","  2/898 [..............................] - ETA: 11:42 - loss: 2.5486 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0769s vs `on_train_batch_end` time: 1.4872s). Check your callbacks.\n","898/898 [==============================] - 72s 80ms/step - loss: 1.4949 - accuracy: 0.4462 - val_loss: 1.2252 - val_accuracy: 0.5300\n","Epoch 2/30\n","898/898 [==============================] - 70s 78ms/step - loss: 1.1435 - accuracy: 0.5667 - val_loss: 1.1449 - val_accuracy: 0.5743\n","Epoch 3/30\n","898/898 [==============================] - 67s 74ms/step - loss: 1.0106 - accuracy: 0.6213 - val_loss: 1.0773 - val_accuracy: 0.5924\n","Epoch 4/30\n","898/898 [==============================] - 69s 77ms/step - loss: 0.9033 - accuracy: 0.6637 - val_loss: 1.1528 - val_accuracy: 0.5832\n","Epoch 5/30\n","898/898 [==============================] - 67s 74ms/step - loss: 0.7833 - accuracy: 0.7090 - val_loss: 1.0623 - val_accuracy: 0.6169\n","Epoch 6/30\n","898/898 [==============================] - 69s 77ms/step - loss: 0.6585 - accuracy: 0.7568 - val_loss: 1.1407 - val_accuracy: 0.6227\n","Epoch 7/30\n","898/898 [==============================] - 68s 75ms/step - loss: 0.5382 - accuracy: 0.8031 - val_loss: 1.1839 - val_accuracy: 0.6172\n","Epoch 8/30\n","898/898 [==============================] - 71s 79ms/step - loss: 0.4152 - accuracy: 0.8481 - val_loss: 1.3462 - val_accuracy: 0.6057\n","Epoch 9/30\n","898/898 [==============================] - 74s 82ms/step - loss: 0.3302 - accuracy: 0.8802 - val_loss: 1.4838 - val_accuracy: 0.6202\n","Epoch 10/30\n","898/898 [==============================] - 72s 80ms/step - loss: 0.2543 - accuracy: 0.9095 - val_loss: 1.6039 - val_accuracy: 0.6099\n","Epoch 11/30\n","898/898 [==============================] - 71s 79ms/step - loss: 0.2084 - accuracy: 0.9274 - val_loss: 1.6939 - val_accuracy: 0.6216\n","Epoch 12/30\n","898/898 [==============================] - 69s 76ms/step - loss: 0.1676 - accuracy: 0.9423 - val_loss: 1.7712 - val_accuracy: 0.6186\n","Epoch 13/30\n","898/898 [==============================] - 71s 80ms/step - loss: 0.1388 - accuracy: 0.9520 - val_loss: 1.8237 - val_accuracy: 0.6300\n","Epoch 14/30\n","898/898 [==============================] - 68s 75ms/step - loss: 0.1163 - accuracy: 0.9599 - val_loss: 1.9747 - val_accuracy: 0.6258\n","Epoch 15/30\n","898/898 [==============================] - 71s 79ms/step - loss: 0.1052 - accuracy: 0.9641 - val_loss: 1.9533 - val_accuracy: 0.6278\n","Epoch 16/30\n","898/898 [==============================] - 69s 77ms/step - loss: 0.0900 - accuracy: 0.9694 - val_loss: 2.1760 - val_accuracy: 0.6305\n","Epoch 17/30\n","898/898 [==============================] - 73s 81ms/step - loss: 0.0815 - accuracy: 0.9726 - val_loss: 2.1141 - val_accuracy: 0.6356\n","Epoch 18/30\n","898/898 [==============================] - 69s 76ms/step - loss: 0.0684 - accuracy: 0.9763 - val_loss: 2.2582 - val_accuracy: 0.6294\n","Epoch 19/30\n","898/898 [==============================] - 73s 81ms/step - loss: 0.0623 - accuracy: 0.9790 - val_loss: 2.2310 - val_accuracy: 0.6280\n","Epoch 20/30\n","898/898 [==============================] - 70s 78ms/step - loss: 0.0559 - accuracy: 0.9809 - val_loss: 2.3385 - val_accuracy: 0.6289\n","Epoch 21/30\n","898/898 [==============================] - 69s 77ms/step - loss: 0.0520 - accuracy: 0.9818 - val_loss: 2.3540 - val_accuracy: 0.6347\n","Epoch 22/30\n","898/898 [==============================] - 71s 79ms/step - loss: 0.0440 - accuracy: 0.9840 - val_loss: 2.4139 - val_accuracy: 0.6344\n","Epoch 23/30\n","898/898 [==============================] - 69s 77ms/step - loss: 0.0438 - accuracy: 0.9854 - val_loss: 2.4022 - val_accuracy: 0.6397\n","Epoch 24/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.0380 - accuracy: 0.9875 - val_loss: 2.4776 - val_accuracy: 0.6361\n","Epoch 25/30\n","898/898 [==============================] - 78s 87ms/step - loss: 0.0351 - accuracy: 0.9880 - val_loss: 2.4716 - val_accuracy: 0.6319\n","Epoch 26/30\n","898/898 [==============================] - 90s 100ms/step - loss: 0.0339 - accuracy: 0.9881 - val_loss: 2.4833 - val_accuracy: 0.6305\n","Epoch 27/30\n","898/898 [==============================] - 82s 91ms/step - loss: 0.0309 - accuracy: 0.9890 - val_loss: 2.5256 - val_accuracy: 0.6303\n","Epoch 28/30\n","898/898 [==============================] - 73s 81ms/step - loss: 0.0300 - accuracy: 0.9895 - val_loss: 2.6239 - val_accuracy: 0.6325\n","Epoch 29/30\n","898/898 [==============================] - 74s 82ms/step - loss: 0.0275 - accuracy: 0.9908 - val_loss: 2.5794 - val_accuracy: 0.6333\n","Epoch 30/30\n","898/898 [==============================] - 79s 87ms/step - loss: 0.0286 - accuracy: 0.9906 - val_loss: 2.5542 - val_accuracy: 0.6358\n","INFO:tensorflow:Assets written to: C:\\Users\\USER\\AppData\\Local\\Temp\\tmp1k1ccehf\\model\\data\\model\\assets\n","Epoch 1/30\n","  2/898 [..............................] - ETA: 18:36 - loss: 3.2214 - accuracy: 0.0781WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1715s vs `on_train_batch_end` time: 2.3167s). Check your callbacks.\n","898/898 [==============================] - 76s 85ms/step - loss: 1.5027 - accuracy: 0.4442 - val_loss: 1.2518 - val_accuracy: 0.5219\n","Epoch 2/30\n","898/898 [==============================] - 74s 83ms/step - loss: 1.1397 - accuracy: 0.5694 - val_loss: 1.1302 - val_accuracy: 0.5857\n","Epoch 3/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.9935 - accuracy: 0.6300 - val_loss: 1.0820 - val_accuracy: 0.6049\n","Epoch 4/30\n","898/898 [==============================] - 67s 75ms/step - loss: 0.8522 - accuracy: 0.6824 - val_loss: 1.0774 - val_accuracy: 0.6063\n","Epoch 5/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.6852 - accuracy: 0.7482 - val_loss: 1.1218 - val_accuracy: 0.6202\n","Epoch 6/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.5296 - accuracy: 0.8060 - val_loss: 1.2163 - val_accuracy: 0.6230\n","Epoch 7/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.3712 - accuracy: 0.8627 - val_loss: 1.3996 - val_accuracy: 0.6339\n","Epoch 8/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.2672 - accuracy: 0.9040 - val_loss: 1.5674 - val_accuracy: 0.6222\n","Epoch 9/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.2104 - accuracy: 0.9250 - val_loss: 1.6750 - val_accuracy: 0.6261\n","Epoch 10/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.1698 - accuracy: 0.9396 - val_loss: 1.7653 - val_accuracy: 0.6291\n","Epoch 11/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.1580 - accuracy: 0.9444 - val_loss: 1.8832 - val_accuracy: 0.6275\n","Epoch 12/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.1589 - accuracy: 0.9462 - val_loss: 1.8413 - val_accuracy: 0.6286\n","Epoch 13/30\n","898/898 [==============================] - 68s 75ms/step - loss: 0.1727 - accuracy: 0.9387 - val_loss: 1.8530 - val_accuracy: 0.6272\n","Epoch 14/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.2137 - accuracy: 0.9243 - val_loss: 1.7229 - val_accuracy: 0.6160\n","Epoch 15/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.2546 - accuracy: 0.9106 - val_loss: 1.7238 - val_accuracy: 0.6216\n","Epoch 16/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.3149 - accuracy: 0.8905 - val_loss: 1.6869 - val_accuracy: 0.5954\n","Epoch 17/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.3698 - accuracy: 0.8679 - val_loss: 1.4667 - val_accuracy: 0.6043\n","Epoch 18/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.4344 - accuracy: 0.8460 - val_loss: 1.3959 - val_accuracy: 0.6041\n","Epoch 19/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.4877 - accuracy: 0.8235 - val_loss: 1.3341 - val_accuracy: 0.5996\n","Epoch 20/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.5449 - accuracy: 0.8035 - val_loss: 1.3551 - val_accuracy: 0.6030\n","Epoch 21/30\n","898/898 [==============================] - 68s 75ms/step - loss: 0.5975 - accuracy: 0.7832 - val_loss: 1.3442 - val_accuracy: 0.5773\n","Epoch 22/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.6214 - accuracy: 0.7769 - val_loss: 1.3415 - val_accuracy: 0.5834\n","Epoch 23/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.6571 - accuracy: 0.7640 - val_loss: 1.3083 - val_accuracy: 0.5729\n","Epoch 24/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.6835 - accuracy: 0.7505 - val_loss: 1.2319 - val_accuracy: 0.6155\n","Epoch 25/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.7256 - accuracy: 0.7371 - val_loss: 1.3120 - val_accuracy: 0.5896\n","Epoch 26/30\n","898/898 [==============================] - 68s 75ms/step - loss: 0.7492 - accuracy: 0.7261 - val_loss: 1.2782 - val_accuracy: 0.5882\n","Epoch 27/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.7496 - accuracy: 0.7262 - val_loss: 1.3074 - val_accuracy: 0.5840\n","Epoch 28/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.7996 - accuracy: 0.7113 - val_loss: 1.2748 - val_accuracy: 0.5676\n","Epoch 29/30\n","898/898 [==============================] - 68s 76ms/step - loss: 0.8142 - accuracy: 0.7066 - val_loss: 1.3679 - val_accuracy: 0.5305\n","Epoch 30/30\n","898/898 [==============================] - 68s 75ms/step - loss: 0.8466 - accuracy: 0.6904 - val_loss: 1.2956 - val_accuracy: 0.5709\n","INFO:tensorflow:Assets written to: C:\\Users\\USER\\AppData\\Local\\Temp\\tmptrpwlp3s\\model\\data\\model\\assets\n"]}],"metadata":{}},{"cell_type":"code","execution_count":10,"source":["print(\"Done\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["Done\n"]}],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}