{"cells":[{"cell_type":"code","execution_count":1,"source":["from tensorflow.keras.applications import mobilenet, mobilenet_v2\n","from sklearn.metrics import confusion_matrix, classification_report\n","from tensorflow.keras.applications import VGG16, ResNet50, ResNet50V2, DenseNet121, DenseNet201\n","from tensorflow.keras.applications import EfficientNetB0, EfficientNetB7, MobileNet, MobileNetV2\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.utils import to_categorical\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import itertools\n","import mlflow.tensorflow\n","import mlflow"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["def prepare_data(data):\n","    \"\"\" Prepare data for modeling \n","        input: data frame with labels und pixel data\n","        output: image and label array \"\"\"\n","\n","    image_array = np.zeros(shape=(len(data), 48, 48, 1))\n","    image_label = np.array(list(map(int, data['emotion'])))\n","\n","    for i, row in enumerate(data.index):\n","        image = np.fromstring(data.loc[row, 'pixels'], dtype=int, sep=' ')\n","        image = np.reshape(image, (48, 48, 1))  # 灰階圖的channel數為1\n","        image_array[i] = image\n","\n","    return image_array, image_label\n","\n","\n","def convert_to_3_channels(img_arrays):\n","    sample_size, nrows, ncols, c = img_arrays.shape\n","    img_stack_arrays = np.zeros((sample_size, nrows, ncols, 3))\n","    for _ in range(sample_size):\n","        img_stack = np.stack(\n","            [img_arrays[_][:, :, 0], img_arrays[_][:, :, 0], img_arrays[_][:, :, 0]], axis=-1)\n","        img_stack_arrays[_] = img_stack/255\n","    return img_stack_arrays\n","\n","\n","def build_model(preModel=VGG16, num_classes=7):\n","\n","    if preModel in [DenseNet121, DenseNet201]:\n","        pred_model = preModel(include_top=False, weights='imagenet',\n","                              input_shape=(48, 48, 3),\n","                              pooling='max')\n","    else:\n","        pred_model = preModel(include_top=False, weights='imagenet',\n","                              input_shape=(48, 48, 3),\n","                              pooling='max', classifier_activation='softmax')\n","    output_layer = Dense(\n","        num_classes, activation=\"softmax\", name=\"output_layer\")\n","\n","    model = tf.keras.Model(\n","        pred_model.inputs, output_layer(pred_model.output))\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(),\n","                  loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n","\n","    return model\n","\n","\n","emotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear',\n","            3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["df_raw = pd.read_csv(\"D:/mycodes/AIFER/data/fer2013.csv\")\n","# 資料切割(訓練、驗證、測試)\n","X_train, y_train = prepare_data(df_raw[df_raw['Usage'] == 'Training'])\n","X_val, y_val = prepare_data(df_raw[df_raw['Usage'] == 'PublicTest'])\n","X_train, X_val = convert_to_3_channels(X_train), convert_to_3_channels(X_val)\n","y_train_oh, y_val_oh = to_categorical(y_train), to_categorical(y_val)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["preModelDict = {\"ResNet50\": ResNet50, \"ResNet50V2\": ResNet50V2, \"DenseNet121\": DenseNet121, \"DenseNet201\": DenseNet201,\n","                \"EfficientNetB0\": EfficientNetB0, \"EfficientNetB7\": EfficientNetB7,\n","                \"MobileNet\": MobileNet, \"MobileNetV2\": MobileNetV2}\n","# 測試模型是否建立成功\n","# for k, v in preModelDict.items():\n","#     model = build_model(preModel=v)\n","#     prob_res = model(X_train[:1]).numpy()\n","#     print(f\"{k} build successfully!\")"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":6,"source":["epochs = 30\n","batch_size = 32\n","preModelDoneList = [\"ResNet50\", \"ResNet50V2\", \"DenseNet121\",\n","                    \"DenseNet201\", \"EfficientNetB0\", \"EfficientNetB7\",\"MobileNet\"]\n","preModel255List = [\"EfficientNetB0\", \"EfficientNetB7\"]  # 0 ~ 255\n","X_train_255 = X_train*255\n","X_val_255 = X_val*255\n","X_train_mobile = mobilenet.preprocess_input(X_train_255)\n","X_val_mobile = mobilenet.preprocess_input(X_val_255)\n","X_train_mobilev2 = mobilenet_v2.preprocess_input(X_train_255)\n","X_val_mobilev2 = mobilenet_v2.preprocess_input(X_val_255)\n","\n","\n","for k, v in preModelDict.items():\n","    if k in preModelDoneList:  # 略過已經訓練好的\n","        continue\n","\n","    if k in preModel255List:  # 需要raw input的模型\n","        model = build_model(preModel=v)\n","        with mlflow.start_run(experiment_id=0, run_name=k+\"_255\"):\n","            mlflow.tensorflow.autolog()\n","            hist1 = model.fit(X_train_255, y_train_oh, validation_data=(X_val_255, y_val_oh),\n","                              epochs=epochs, batch_size=batch_size)\n","        mlflow.end_run()\n","\n","    if k == \"MobileNetV2\":  # 需要raw input的模型\n","        model = build_model(preModel=v)\n","        with mlflow.start_run(experiment_id=0, run_name=k+\"_neg\"):\n","            mlflow.tensorflow.autolog()\n","            hist1 = model.fit(X_train_mobilev2, y_train_oh, validation_data=(X_val_mobilev2, y_val_oh),\n","                              epochs=epochs, batch_size=batch_size)\n","        mlflow.end_run()\n","\n","    if k == \"MobileNet\":  # 需要raw input的模型\n","        model = build_model(preModel=v)\n","        with mlflow.start_run(experiment_id=0, run_name=k+\"_neg\"):\n","            mlflow.tensorflow.autolog()\n","            hist1 = model.fit(X_train_mobile, y_train_oh, validation_data=(X_val_mobile, y_val_oh),\n","                              epochs=epochs, batch_size=batch_size)\n","        mlflow.end_run()\n","    else:\n","        model = build_model(preModel=v)\n","        with mlflow.start_run(experiment_id=0, run_name=k):\n","            mlflow.tensorflow.autolog()\n","            hist1 = model.fit(X_train, y_train_oh, validation_data=(X_val, y_val_oh),\n","                              epochs=epochs, batch_size=batch_size)\n","        mlflow.end_run()"],"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n","Epoch 1/30\n","  1/898 [..............................] - ETA: 0s - loss: 3.9241 - accuracy: 0.1250WARNING:tensorflow:From C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n","Instructions for updating:\n","use `tf.profiler.experimental.stop` instead.\n","  2/898 [..............................] - ETA: 28:21 - loss: 3.7824 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0389s vs `on_train_batch_end` time: 3.3628s). Check your callbacks.\n","898/898 [==============================] - 77s 86ms/step - loss: 1.7893 - accuracy: 0.3170 - val_loss: 3.0977 - val_accuracy: 0.2494\n","Epoch 2/30\n","898/898 [==============================] - 39s 44ms/step - loss: 1.8004 - accuracy: 0.2981 - val_loss: 2.1397 - val_accuracy: 0.2455\n","Epoch 3/30\n","898/898 [==============================] - 41s 46ms/step - loss: 1.7599 - accuracy: 0.3152 - val_loss: 3.3266 - val_accuracy: 0.1819\n","Epoch 4/30\n","898/898 [==============================] - 38s 43ms/step - loss: 1.7464 - accuracy: 0.3236 - val_loss: 2.1185 - val_accuracy: 0.1463\n","Epoch 5/30\n","898/898 [==============================] - 38s 42ms/step - loss: 1.7168 - accuracy: 0.3354 - val_loss: 4.8927 - val_accuracy: 0.2494\n","Epoch 6/30\n","898/898 [==============================] - 39s 44ms/step - loss: 1.7323 - accuracy: 0.3291 - val_loss: 2.1366 - val_accuracy: 0.2494\n","Epoch 7/30\n","898/898 [==============================] - 38s 43ms/step - loss: 1.7043 - accuracy: 0.3476 - val_loss: 1.9347 - val_accuracy: 0.1714\n","Epoch 8/30\n","898/898 [==============================] - 43s 48ms/step - loss: 1.7487 - accuracy: 0.3274 - val_loss: 2.4905 - val_accuracy: 0.1156\n","Epoch 9/30\n","898/898 [==============================] - 36s 40ms/step - loss: 1.6839 - accuracy: 0.3517 - val_loss: 3.6466 - val_accuracy: 0.1691\n","Epoch 10/30\n","898/898 [==============================] - 39s 43ms/step - loss: 1.6838 - accuracy: 0.3526 - val_loss: 1.9586 - val_accuracy: 0.1753\n","Epoch 11/30\n","898/898 [==============================] - 36s 40ms/step - loss: 1.7645 - accuracy: 0.3161 - val_loss: 2.0012 - val_accuracy: 0.2494\n","Epoch 12/30\n","898/898 [==============================] - 45s 50ms/step - loss: 1.7780 - accuracy: 0.3111 - val_loss: 2.9937 - val_accuracy: 0.2494\n","Epoch 13/30\n","898/898 [==============================] - 46s 51ms/step - loss: 1.7475 - accuracy: 0.3267 - val_loss: 4.5317 - val_accuracy: 0.2009\n","Epoch 14/30\n","898/898 [==============================] - 44s 49ms/step - loss: 1.7637 - accuracy: 0.3246 - val_loss: 2.1787 - val_accuracy: 0.1878\n","Epoch 15/30\n","898/898 [==============================] - 52s 58ms/step - loss: 1.7682 - accuracy: 0.3177 - val_loss: 2.6427 - val_accuracy: 0.2494\n","Epoch 16/30\n","898/898 [==============================] - 42s 46ms/step - loss: 1.8329 - accuracy: 0.2796 - val_loss: 1.8279 - val_accuracy: 0.2494\n","Epoch 17/30\n","898/898 [==============================] - 53s 59ms/step - loss: 1.7903 - accuracy: 0.3029 - val_loss: 1.8872 - val_accuracy: 0.1836\n","Epoch 18/30\n","898/898 [==============================] - 39s 43ms/step - loss: 1.7574 - accuracy: 0.3212 - val_loss: 1.9145 - val_accuracy: 0.2909\n","Epoch 19/30\n","898/898 [==============================] - 41s 45ms/step - loss: 1.7501 - accuracy: 0.3192 - val_loss: 2.2229 - val_accuracy: 0.2494\n","Epoch 20/30\n","898/898 [==============================] - 41s 46ms/step - loss: 1.7433 - accuracy: 0.3240 - val_loss: 2.0209 - val_accuracy: 0.1819\n","Epoch 21/30\n","898/898 [==============================] - 42s 47ms/step - loss: 1.7057 - accuracy: 0.3408 - val_loss: 2.1774 - val_accuracy: 0.1694\n","Epoch 22/30\n","898/898 [==============================] - 38s 43ms/step - loss: 1.6862 - accuracy: 0.3575 - val_loss: 2.0444 - val_accuracy: 0.1819\n","Epoch 23/30\n","898/898 [==============================] - 43s 47ms/step - loss: 1.7203 - accuracy: 0.3410 - val_loss: 2.3227 - val_accuracy: 0.2218\n","Epoch 24/30\n","898/898 [==============================] - 44s 49ms/step - loss: 1.8170 - accuracy: 0.2850 - val_loss: 1.9581 - val_accuracy: 0.1156\n","Epoch 25/30\n","898/898 [==============================] - 38s 42ms/step - loss: 1.8104 - accuracy: 0.2917 - val_loss: 1.8919 - val_accuracy: 0.1605\n","Epoch 26/30\n","898/898 [==============================] - 45s 50ms/step - loss: 1.7652 - accuracy: 0.3118 - val_loss: 4.1052 - val_accuracy: 0.1691\n","Epoch 27/30\n","898/898 [==============================] - 54s 60ms/step - loss: 1.7794 - accuracy: 0.3055 - val_loss: 5.5531 - val_accuracy: 0.1691\n","Epoch 28/30\n","898/898 [==============================] - 37s 41ms/step - loss: 1.7677 - accuracy: 0.3143 - val_loss: 1.8793 - val_accuracy: 0.1867\n","Epoch 29/30\n","898/898 [==============================] - 44s 48ms/step - loss: 1.7729 - accuracy: 0.3112 - val_loss: 2.8276 - val_accuracy: 0.1691\n","Epoch 30/30\n","898/898 [==============================] - 52s 57ms/step - loss: 1.7898 - accuracy: 0.3003 - val_loss: 1.7272 - val_accuracy: 0.3020\n","WARNING:tensorflow:From C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","WARNING:tensorflow:From C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n","INFO:tensorflow:Assets written to: C:\\Users\\USER\\AppData\\Local\\Temp\\tmpr_91q5sx\\model\\data\\model\\assets\n","WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n","Epoch 1/30\n","  2/898 [..............................] - ETA: 12:09 - loss: 2.9607 - accuracy: 0.1562WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0658s vs `on_train_batch_end` time: 1.5612s). Check your callbacks.\n","898/898 [==============================] - 54s 61ms/step - loss: 1.7505 - accuracy: 0.3459 - val_loss: 2.4093 - val_accuracy: 0.2215\n","Epoch 2/30\n","898/898 [==============================] - 42s 47ms/step - loss: 1.5552 - accuracy: 0.4154 - val_loss: 1.7524 - val_accuracy: 0.3759\n","Epoch 3/30\n","898/898 [==============================] - 38s 42ms/step - loss: 1.5795 - accuracy: 0.4122 - val_loss: 2.2267 - val_accuracy: 0.2140\n","Epoch 4/30\n","898/898 [==============================] - 38s 42ms/step - loss: 1.6048 - accuracy: 0.3980 - val_loss: 2.4088 - val_accuracy: 0.3480\n","Epoch 5/30\n","898/898 [==============================] - 38s 42ms/step - loss: 1.5270 - accuracy: 0.4375 - val_loss: 4.9971 - val_accuracy: 0.2552\n","Epoch 6/30\n","898/898 [==============================] - 40s 45ms/step - loss: 1.5973 - accuracy: 0.4024 - val_loss: 2.5977 - val_accuracy: 0.2497\n","Epoch 7/30\n","898/898 [==============================] - 39s 44ms/step - loss: 1.7124 - accuracy: 0.3454 - val_loss: 5.2571 - val_accuracy: 0.2488\n","Epoch 8/30\n","898/898 [==============================] - 41s 46ms/step - loss: 1.6219 - accuracy: 0.3914 - val_loss: 3.1223 - val_accuracy: 0.1480\n","Epoch 9/30\n","898/898 [==============================] - 44s 49ms/step - loss: 1.6873 - accuracy: 0.3666 - val_loss: 3.9551 - val_accuracy: 0.1337\n","Epoch 10/30\n","898/898 [==============================] - 41s 46ms/step - loss: 1.6000 - accuracy: 0.3998 - val_loss: 2.2100 - val_accuracy: 0.3313\n","Epoch 11/30\n","898/898 [==============================] - 50s 56ms/step - loss: 1.5352 - accuracy: 0.4219 - val_loss: 2.6254 - val_accuracy: 0.3034\n","Epoch 12/30\n","898/898 [==============================] - 36s 40ms/step - loss: 1.7808 - accuracy: 0.3199 - val_loss: 2.0103 - val_accuracy: 0.2700\n","Epoch 13/30\n","898/898 [==============================] - 54s 60ms/step - loss: 1.6880 - accuracy: 0.3580 - val_loss: 2.9754 - val_accuracy: 0.2895\n","Epoch 14/30\n","898/898 [==============================] - 41s 45ms/step - loss: 1.6228 - accuracy: 0.3868 - val_loss: 1.6255 - val_accuracy: 0.3906\n","Epoch 15/30\n","898/898 [==============================] - 45s 50ms/step - loss: 1.5802 - accuracy: 0.3989 - val_loss: 1.6049 - val_accuracy: 0.3906\n","Epoch 16/30\n","898/898 [==============================] - 47s 53ms/step - loss: 1.6074 - accuracy: 0.3914 - val_loss: 2.4739 - val_accuracy: 0.3009\n","Epoch 17/30\n","898/898 [==============================] - 38s 43ms/step - loss: 1.6733 - accuracy: 0.3633 - val_loss: 2.1000 - val_accuracy: 0.3118\n","Epoch 18/30\n","898/898 [==============================] - 61s 68ms/step - loss: 1.7300 - accuracy: 0.3332 - val_loss: 2.0057 - val_accuracy: 0.2516\n","Epoch 19/30\n","898/898 [==============================] - 37s 41ms/step - loss: 1.7633 - accuracy: 0.3279 - val_loss: 2.0136 - val_accuracy: 0.2575\n","Epoch 20/30\n","898/898 [==============================] - 43s 48ms/step - loss: 1.6878 - accuracy: 0.3602 - val_loss: 1.8114 - val_accuracy: 0.2976\n","Epoch 21/30\n","898/898 [==============================] - 48s 53ms/step - loss: 1.7037 - accuracy: 0.3512 - val_loss: 2.2432 - val_accuracy: 0.2310\n","Epoch 22/30\n","898/898 [==============================] - 38s 42ms/step - loss: 1.6056 - accuracy: 0.3911 - val_loss: 1.8635 - val_accuracy: 0.3369\n","Epoch 23/30\n","898/898 [==============================] - 41s 46ms/step - loss: 1.6435 - accuracy: 0.3771 - val_loss: 1.9260 - val_accuracy: 0.2979\n","Epoch 24/30\n","898/898 [==============================] - 44s 49ms/step - loss: 1.5907 - accuracy: 0.4034 - val_loss: 1.6803 - val_accuracy: 0.3603\n","Epoch 25/30\n","898/898 [==============================] - 42s 47ms/step - loss: 1.6360 - accuracy: 0.3902 - val_loss: 2.4853 - val_accuracy: 0.3305\n","Epoch 26/30\n","898/898 [==============================] - 49s 55ms/step - loss: 1.6182 - accuracy: 0.3893 - val_loss: 1.8251 - val_accuracy: 0.3402\n","Epoch 27/30\n","898/898 [==============================] - 43s 48ms/step - loss: 1.6172 - accuracy: 0.3951 - val_loss: 1.8032 - val_accuracy: 0.3895\n","Epoch 28/30\n","898/898 [==============================] - 43s 48ms/step - loss: 1.5801 - accuracy: 0.4075 - val_loss: 1.7042 - val_accuracy: 0.3764\n","Epoch 29/30\n","898/898 [==============================] - 46s 51ms/step - loss: 1.5431 - accuracy: 0.4217 - val_loss: 1.5585 - val_accuracy: 0.4099\n","Epoch 30/30\n","898/898 [==============================] - 38s 42ms/step - loss: 1.5465 - accuracy: 0.4217 - val_loss: 1.6632 - val_accuracy: 0.3845\n","INFO:tensorflow:Assets written to: C:\\Users\\USER\\AppData\\Local\\Temp\\tmp25akobbx\\model\\data\\model\\assets\n"]}],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}